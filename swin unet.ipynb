{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5a71d-1453-4f8e-ae03-b8953f6cbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# pd.options.plotting.backend = \"plotly\"\n",
    "import random\n",
    "from glob import glob\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import copy\n",
    "# import joblib\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from IPython import display as ipd\n",
    "\n",
    "# visualization\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Sklearn\n",
    "# from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "#torchio\n",
    "import torchio as tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce990be-8a4c-4248-a8e9-5b7a02d7e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class indices\n",
    "\n",
    "LARGE_BOWEL = 0\n",
    "SMALL_BOWEL = 1\n",
    "STOMACH = 2\n",
    "MASK_INDICES = {'large_bowel': LARGE_BOWEL, 'small_bowel':SMALL_BOWEL, 'stomach':STOMACH}\n",
    "DATA_SIZE = 219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be45d59f-7ed0-4000-b351-6ad3564f64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed          = 101\n",
    "    debug         = False # set debug=False for Full Training\n",
    "    train_bs      = 1\n",
    "    valid_bs      = 1\n",
    "    image_size    = [128, 128, 128] # Depth, Width, Height\n",
    "    whf_pool_size = 4\n",
    "    epochs        = 64\n",
    "    lr            = 2e-3\n",
    "    scheduler     = 'CosineAnnealingLR'\n",
    "    warmup_epochs = 0\n",
    "    wd            = 1e-3\n",
    "    n_accumulate  = max(1, 32//train_bs)\n",
    "    n_fold        = 5\n",
    "    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    min_lr        = 1e-6\n",
    "    T_max         = int(DATA_SIZE/train_bs/n_accumulate*epochs)+50\n",
    "    T_0           = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64c504-b724-4d4c-8a2a-b9678ee25959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print('> SEEDING DONE')\n",
    "    \n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f546e0-a183-403f-854d-c7e60d375083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scans(scan_path):\n",
    "    image_files = [f for f in os.listdir(scan_path) if os.path.isfile(os.path.join(scan_path, f))]\n",
    "    image_files.sort(key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    scan_slices = []\n",
    "    for img_file in image_files:\n",
    "        img_file_path = os.path.join(scan_path, img_file)\n",
    "        scan_slice = cv2.imread(img_file_path, cv2.IMREAD_UNCHANGED)\n",
    "        scan_slice = scan_slice.astype('float32')\n",
    "        scan_slices.append(scan_slice)\n",
    "    \n",
    "    img = np.stack(scan_slices)\n",
    "    max_val = np.max(img)\n",
    "    if max_val:\n",
    "        img /= max_val\n",
    "\n",
    "    return img\n",
    "\n",
    "def load_mask(path):\n",
    "    mask = np.load(path).transpose([3,0,1,2])\n",
    "    mask = mask.astype('float32')\n",
    "    return mask\n",
    "\n",
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8a770-7c7e-433e-9369-16cfb65011f0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a7903e-7894-4579-906e-8abfafcf2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(imgs, msks, depth, size=3):\n",
    "    plt.figure(figsize=(5*5, 5))\n",
    "    for idx in range(size):\n",
    "        plt.subplot(1, 5, idx+1)\n",
    "        img = imgs[idx, depth].permute((1, 2, 0)).numpy()*255.0\n",
    "        img = img.astype('uint8')\n",
    "        msk = msks[idx, depthm].permute((1, 2, 0)).numpy()*255.0\n",
    "        show_img(img, msk)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6b56f-1000-4978-8912-a6f9c38a4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_dimension_order_to_c_z_x_y = tio.Lambda(lambda x: torch.permute(x,(0,3,1,2)), types_to_apply=[tio.INTENSITY, tio.LABEL])\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": tio.Compose(\n",
    "        [tio.RescaleIntensity(out_min_max=(0, 1)), \n",
    "         tio.Resize(target_shape=CFG.image_size, image_interpolation='linear', label_interpolation='nearest'), \n",
    "         tio.RandomFlip(axes=(0,1), p=0.2), \n",
    "         tio.RandomAffine(scales=(0.9, 1.2), degrees=15, p=0.2), \n",
    "         tio.OneOf([tio.RandomElasticDeformation(), tio.RandomMotion()], p=0.3), \n",
    "         tio.OneOf([tio.RandomGhosting(), tio.RandomSpike(), tio.RandomBlur()], p=0.3),\n",
    "         # set_dimension_order_to_c_z_x_y\n",
    "        ]\n",
    "    ),\n",
    "    \"valid\": tio.Compose(\n",
    "        [tio.RescaleIntensity(out_min_max=(0, 1)), \n",
    "         tio.Resize(target_shape=CFG.image_size, image_interpolation='linear', label_interpolation='nearest'),\n",
    "         # set_dimension_order_to_c_z_x_y\n",
    "        ]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d9e82-c512-4c68-b0e6-e8a1bc58a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizedSubject(tio.Subject):\n",
    "    def load(self):\n",
    "        super(ResizedSubject, self).load()\n",
    "        self['original_size'] = list(self['scan'][tio.DATA].shape)[1:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1628d-bb80-4c59-b5eb-f70045bc22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_reader(scan_path):\n",
    "    image_files = [f for f in os.listdir(scan_path) if os.path.isfile(os.path.join(scan_path, f)) and f.endswith('.png')]\n",
    "    image_files.sort(key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    scan_slices = []\n",
    "    for img_file in image_files:\n",
    "        img_file_path = os.path.join(scan_path, img_file)\n",
    "        scan_slice = cv2.imread(img_file_path, cv2.IMREAD_UNCHANGED)\n",
    "        scan_slice = np.expand_dims(scan_slice, axis=0)\n",
    "        scan_slice = scan_slice.astype('float32')\n",
    "        scan_slices.append(scan_slice)\n",
    "    \n",
    "    img = np.stack(scan_slices, axis=-1)\n",
    "\n",
    "    return img, None\n",
    "\n",
    "def mask_reader(path):\n",
    "    mask = np.load(path).transpose([3,0,1,2])\n",
    "    mask = mask.astype('float32')\n",
    "    return mask, None\n",
    "\n",
    "def get_subject(case_day, scan_path, mask_path=None):\n",
    "    \n",
    "    if not mask_path:\n",
    "        return ResizeSubject(\n",
    "            case_day=case_day, \n",
    "            scan=tio.ScalarImage(path=scan_path, reader=scan_reader),\n",
    "        )\n",
    "\n",
    "    return ResizedSubject(\n",
    "        case_day=case_day,\n",
    "        scan=tio.ScalarImage(path=scan_path, reader=scan_reader),\n",
    "        label=tio.LabelMap(path=mask_path, reader=mask_reader),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9c427-b0ee-41cf-98dd-d4263934ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for assigning fold to Data frame rows\n",
    "\n",
    "def assign_fold_to_df(df, num_folds=5):\n",
    "    num_rows = len(df)\n",
    "    folds = np.zeros(num_rows, dtype='uint8')\n",
    "    fold_vals = [i for i in range(num_folds)]\n",
    "    indices = [i for i in range(num_rows)]\n",
    "    random.shuffle(indices)\n",
    "    random.shuffle(fold_vals)\n",
    "    for i in indices:\n",
    "        folds[i] = fold_vals[i % num_folds]\n",
    "\n",
    "     # not shuffling fold_vals will cause some fold indices to always get less number of items than other fold indices.\n",
    "    # EX: number of items is 2, and num_folds = 3. Assigning fold index with order [0,1,2] will make only fold-0 and fold-1 get an item, while fold-2 has 0 item.\n",
    " \n",
    "    df['fold'] = folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6d215-6498-4405-809b-b190a0f07734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(df, fold, debug=False):\n",
    "    train_df = df.query(\"fold!=@fold\").reset_index(drop=True)\n",
    "    valid_df = df.query(\"fold==@fold\").reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = tio.SubjectsDataset(\n",
    "        [get_subject(row['case_day'], row['image_path'], row['mask_path']) for _, row in train_df.iterrows()], \n",
    "        transform=data_transforms['train']\n",
    "    )\n",
    "    valid_dataset = tio.SubjectsDataset(\n",
    "        [get_subject(row['case_day'], row['image_path'], row['mask_path']) for _, row in valid_df.iterrows()], \n",
    "        transform=data_transforms['train']\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.train_bs if not debug else 1, \n",
    "                              num_workers=4, shuffle=True, pin_memory=False, drop_last=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs if not debug else 1, \n",
    "                              num_workers=4, shuffle=False, pin_memory=False)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b5a7f-c5f8-4f8a-81ac-a7d73803ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data frame\n",
    "\n",
    "df_train = pd.read_csv('./input/uw-madison-gi-tract-image-segmentation/train.csv')\n",
    "train_path = './input/uw-madison-gi-tract-image-segmentation/train'\n",
    "df_train['case_day'] = df_train['id'].map(lambda x: x.split('_slice')[0])\n",
    "df_train.drop(columns=['class', 'segmentation', 'id'], inplace=True)\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "df_train['image_path'] = df_train['case_day'].map(lambda x: f'{train_path}/' + x.split('_')[0] + f'/{x}/scans')\n",
    "df_train['mask_path'] =  df_train['case_day'].map(lambda x: f'{train_path}/' + x.split('_')[0] + f'/{x}/masks3D/{x}.npy')\n",
    "assign_fold_to_df(df_train)\n",
    "df_train.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d045a67-a7d7-4ca7-9dcb-34eb4856b3df",
   "metadata": {},
   "source": [
    "### Compare with Transformed Data\n",
    "\n",
    "Create Dataset and compare with the transformed image.\\\n",
    "You may want to temporaruily remove any spatial transformation that moves the original data along z-axis, \\\n",
    "because bellow code compares image with fixed slice number(z-axis value).\n",
    "\n",
    "Skip bellow section, if you are done dataset debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc84380-6f2e-4260-8306-8411e0c19f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a7d70-b057-4dea-9c32-3815c727b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dataset = tio.SubjectsDataset(\n",
    "        [get_subject(row['case_day'], row['image_path'], row['mask_path']) for _, row in df_train.iterrows()], \n",
    "        transform=data_transforms['train']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db98b53-48fa-4def-ad49-c728bdecb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subject\n",
    "# By selecting an item with indexing, data transformation is perofrmed in bellow code.\n",
    "# It will be performed again if you execute bellow code again even with the same index value.\n",
    "subject = subject_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f77a9-713b-4023-800c-3635419950da",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_num = 113 # z-axis value\n",
    "case_day = subject['case_day']\n",
    "original_size = subject['original_size']\n",
    "case_day, original_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4d747-7ffd-4956-b59d-b27a0ef377b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the original image\n",
    "\n",
    "df_row = df_train.loc[df_train['case_day'] == case_day].iloc[0]\n",
    "image_path = glob.glob(df_row['image_path'] + f'/slice_{str(slice_num).zfill(4)}_*.png')[0]\n",
    "image = plt.imread(image_path)\n",
    "mask = load_mask(df_row['mask_path'])[:, :, :, slice_num-1]\n",
    "mask[1] *= 2\n",
    "mask[2] *= 3\n",
    "mask_sum = mask.sum(axis=0)\n",
    "fig, ax = plt.subplots(1,3,figsize=(10,6))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(mask_sum)\n",
    "ax[2].imshow(image,'gray')\n",
    "ax[2].imshow(mask_sum, alpha=0.5)\n",
    "plt.show()\n",
    "mask.sum(axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c693f6a-f803-4eac-a44e-658d5b3a2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show trnsformed image\n",
    "resized_slice_num = slice_num * CFG.image_size[2] // len(glob.glob(df_row['image_path'] + f'/slice_*.png')) \n",
    "image = subject['scan'][tio.DATA][0][:,:, resized_slice_num -1]\n",
    "mask = subject['label'][tio.DATA][:,:,:, resized_slice_num - 1]\n",
    "mask[1] *= 2\n",
    "mask[2] *= 3\n",
    "mask_sum = mask.sum(axis=0)\n",
    "fig, ax = plt.subplots(1,3,figsize=(10,6))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(mask_sum)\n",
    "ax[2].imshow(image,'gray')\n",
    "ax[2].imshow(mask_sum, alpha=0.5)\n",
    "plt.show()\n",
    "mask.sum(axis=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca055d-8dd8-4194-a469-36c0a1bf4b87",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8312d-9cb8-4d12-8a1d-69aa974aec9b",
   "metadata": {},
   "source": [
    "### Cycle shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb734b8-6988-4963-ab18-3b42e1dea048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape (b, 56, 56, 96)\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement, self.displacement), dims=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc79ed-ee69-4bfe-a71f-90d24e195e30",
   "metadata": {},
   "source": [
    "### Skip Connection & Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adfdbd-9e19-425a-8501-131d464b0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f214935-8794-46e9-9b9e-d475828956f2",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f099049-46b4-4bff-bf7d-8d9f33720ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915bc44c-8e7b-466d-a963-140ececa7632",
   "metadata": {},
   "source": [
    "### Attetion Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da3198-d6fd-4789-8bda-fe815da9c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expects a tensor that has same shape as the window, and have values 1 or -1 only\n",
    "def _get_mask_from_bipartitioned_window(window_partition):\n",
    "    # flatten the window with partitioned value\n",
    "    window_partition = window_partition.view(-1)\n",
    "\n",
    "    # tensor with size (window_size ** 3, window_size ** 3), where each value corresponds to a pair of points in the window\n",
    "    # if the point pair is in the same partition, it will have value 1, and -1 otherwise.\n",
    "    mask_base = (window_partition[:, None] @ window_partition[None,: ])\n",
    "\n",
    "    # change value -1 to 0, and 1 to 2\n",
    "    mask_base +=1\n",
    "    # covert value 0 to nan and 1 to negative infinity\n",
    "    mask_base *= -float('inf')\n",
    "    \n",
    "    # mask_base = (window_partition[:, None] @ window_partition[None,: ]) : tensor of (window_size ** 3, window_size ** 3, with value 1 or -1\n",
    "    # mask_base += 1 :converts value -1 to 0, and value 1 to 2\n",
    "    # mask_base *= float('-inf') : converts value 0 to nan and value 2 to negative infinity\n",
    "    # mask_base *= 2 : tensor of (window_size ** 3, window_size ** 3, with value 2 or 0\n",
    "    mask_base = ((window_partition[:, None] @ window_partition[None,: ]) + 1) * float('-inf') \n",
    "\n",
    "    # mask = torch.nan_to_num(mask_base) : converts nan value to zero, and netative infinity value to the lowest value of float data type \n",
    "    # mask *= 2 : converts lowest value back to negative infinity \n",
    "    return torch.nan_to_num(mask_base) * 2\n",
    "\n",
    "def create_mask(window_size, displacement, left_right, upper_lower, front_back):\n",
    "    # assume spatial dimension has (order, width, height, depth)\n",
    "    \n",
    "    # masked positions will have value negative infinite\n",
    "    # unmasked positions will have value zero\n",
    "    mask = torch.zeros(window_size ** 3, window_size ** 3)\n",
    "    \n",
    "    if left_right:\n",
    "        window_partition = torch.zeros(window_size, window_size, window_size)\n",
    "        # partition window by value -1 and 1 \n",
    "        window_partition[:-displacement] = 1\n",
    "        window_partition[-displacement:] = -1\n",
    "        \n",
    "        mask += _get_mask_from_bipartitioned_window(window_partition)\n",
    "    \n",
    "    if upper_lower:\n",
    "        window_partition = torch.zeros(window_size, window_size, window_size)\n",
    "        # partition window by value -1 and 1 \n",
    "        window_partition[:, :-displacement] = 1\n",
    "        window_partition[:, -displacement:] = -1\n",
    "        \n",
    "        mask += _get_mask_from_bipartitioned_window(window_partition)\n",
    "        \n",
    "    if front_back:\n",
    "        window_partition = torch.zeros(window_size, window_size, window_size)\n",
    "        # partition window by value -1 and 1 \n",
    "        window_partition[:, :, :-displacement] = 1\n",
    "        window_partition[:, :, -displacement:] = -1\n",
    "        \n",
    "        mask += _get_mask_from_bipartitioned_window(window_partition)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462b158-1af9-4977-9781-5e41229153bd",
   "metadata": {},
   "source": [
    "### W-MSA & SW-MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1dcc9-5354-477f-a7f5-c1a13c6a639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y, z] for x in range(window_size) for y in range(window_size) for z in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding=True, attention_drop=0.0, output_drop=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "        \n",
    "        self.attention_drop = nn.Dropout(attention_drop)\n",
    "        self.output_drop = nn.Dropout(output_drop)\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2  # 7//2 = 3\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            self.upper_lower_mask = nn.Parameter(\n",
    "                create_mask(\n",
    "                    window_size=window_size, \n",
    "                    displacement=displacement, \n",
    "                    upper_lower=True, \n",
    "                    left_right=False, \n",
    "                    front_back=False\n",
    "                ), \n",
    "                requires_grad=False\n",
    "            )\n",
    "            self.left_right_mask = nn.Parameter(\n",
    "                create_mask(\n",
    "                    window_size=window_size, \n",
    "                    displacement=displacement, \n",
    "                    upper_lower=False, \n",
    "                    left_right=True, \n",
    "                    front_back=False\n",
    "                ), \n",
    "                requires_grad=False\n",
    "            )\n",
    "            self.front_back_mask = nn.Parameter(\n",
    "                create_mask(\n",
    "                    window_size=window_size, \n",
    "                    displacement=displacement, \n",
    "                    upper_lower=False, \n",
    "                    left_right=False, \n",
    "                    front_back=True\n",
    "                ), \n",
    "                requires_grad=False\n",
    "            )\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            #  + window_size - 1 is for removing negative values\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1, 2 * window_size - 1))\n",
    "            \n",
    "            # EX window size is 4, query has postion (1, 1, 1) and key has position (1, 1, 1)\n",
    "            # then query -> key has relative distance (3, 3, 3) = (0, 0, 0) + (window size -1)\n",
    "            # query (0, 0, 0),  key (3, 3, 3)\n",
    "            # then query -> key relative distance = (6, 6, 6) = (3, 3, 3) + (4 - 1)\n",
    "            # query (3, 3, 3), key (0, 0, 0)\n",
    "            # then query -> key relative distance = (0, 0, 0) = (-3, -3, -3) + (4 - 1)\n",
    "            \n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 3, window_size ** 3))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)\n",
    "\n",
    "        batch_size, width, height, depth, _ = x.shape\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)  # (b, 56, 56, 56, 288) -> tuple len 3 (b, 56, 56, 56, 96)\n",
    "        nw_h = height // self.window_size  # 8\n",
    "        nw_w = width // self.window_size  # 8\n",
    "        nw_d = depth // self.window_size  # 8\n",
    "        \n",
    "        # 'b (nw_w w_w) (nw_h w_h) (nw_d w_d) (h d) -> b h (nw_w nw_h nw_d) (w_w w_h w_d) d'\n",
    "        q, k, v = map(\n",
    "            lambda t: t.view(\n",
    "                batch_size, nw_w, self.window_size, nw_h, self.window_size, nw_d, self.window_size, self.heads, -1\n",
    "            ).permute(0, 8, 1, 3, 5, 2, 4, 6, 7).reshape(batch_size, self.heads, nw_w * nw_h * nw_d, self.window_size ** 3, -1), \n",
    "            qkv\n",
    "        ) \n",
    "\n",
    "\n",
    "        # (b, 3, 8 ** 3, 7 ** 3, 32), (b, 3, 8 ** 3, 7 ** 3, 32)  -> (b, 3, 8 ** 3, 7 ** 3, 7 ** 3)\n",
    "        dots = torch.einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[\n",
    "                self.relative_indices[:, :, 0].type(torch.long), \n",
    "                self.relative_indices[:, :, 1].type(torch.long), \n",
    "                self.relative_indices[:, :, 2].type(torch.long)\n",
    "            ]  # (7 **3, 7 **3)\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "\n",
    "        if self.shifted:  # masking\n",
    "            dots = dots.view(batch_size, self.heads, nw_w, nw_h, nw_d, self.window_size ** 3, self.window_size **3)\n",
    "            dots[:, :, -1] += self.left_right_mask\n",
    "            dots[:, :, :, -1] += self.upper_lower_mask\n",
    "            dots[:, :, :, :, -1] += self.front_back_mask\n",
    "            dots = dots.view(batch_size, self.heads, -1, self.window_size ** 3, self.window_size **3)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        \n",
    "        # 'b h (w_h w_w) (nw_w nw_h nw_d) (w_w w_h w_d) d -> b (nw_w w_w) (nw_h w_h) (nw_d w_d) (h d)\n",
    "        out = out.view(\n",
    "            batch_size, self.heads, nw_w, nw_h, nw_d, self.window_size, self.window_size, self.window_size, -1\n",
    "        ).permute(\n",
    "            (0, 2, 5, 3, 6, 4, 7, 1, 8)\n",
    "        ).reshape(\n",
    "            batch_size, nw_w * self.window_size, nw_h * self.window_size, nw_d * self.window_size, -1\n",
    "        )\n",
    "                        \n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)  # shift한 값을 원래 위치로\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572e3df-de63-470d-b26f-02a23bd68515",
   "metadata": {},
   "source": [
    "### Swin Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff1601-1697-43bd-a040-d7cac8aa75bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim, \n",
    "        heads, \n",
    "        head_dim, \n",
    "        mlp_dim, \n",
    "        shifted, \n",
    "        window_size, \n",
    "        relative_pos_embedding=True,\n",
    "        attention_drop_rate=0.0,\n",
    "        wa_output_drop_rate=0.0,\n",
    "        ff_drop_rate=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(\n",
    "            PreNorm(\n",
    "                dim, \n",
    "                WindowAttention(\n",
    "                    dim=dim,\n",
    "                    heads=heads,\n",
    "                    head_dim=head_dim,\n",
    "                    shifted=shifted,\n",
    "                    window_size=window_size,\n",
    "                    relative_pos_embedding=relative_pos_embedding, \n",
    "                    attention_drop= attention_drop_rate, \n",
    "                    output_drop=wa_output_drop_rate\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim, drop=ff_drop_rate)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962ac92-7e43-43a9-88a3-5ba8c3eeb490",
   "metadata": {},
   "source": [
    "### Patch Partition or Patch Merging & Linear Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8cb96-c7b5-4435-b9f3-bc5c19345420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 3, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, w, h, d = x.shape\n",
    "        new_w, new_h, new_d = w // self.downscaling_factor, h // self.downscaling_factor, d // self.downscaling_factor  #  num patches along width, height, depth\n",
    "        x = x.view(b, c, new_w, self.downscaling_factor, new_h, self.downscaling_factor, new_d, self.downscaling_factor).permute(0, 2, 4, 6, 3, 5, 7, 1).reshape(b, new_w, new_h, new_d, -1)\n",
    "        return self.linear(x)  # (b, 56, 56, 56, 192) -> (b, 56, 56, 56, out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2629e0b-8a73-4237-9803-9da039a116a6",
   "metadata": {},
   "source": [
    "### StageModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a8f65-97a8-45c8-9875-f1c55dcfea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        hidden_dimension, \n",
    "        layers, \n",
    "        downscaling_factor, \n",
    "        num_heads, \n",
    "        head_dim, \n",
    "        window_size,\n",
    "        relative_pos_embedding=True,\n",
    "        attention_drop_rate=0.0,\n",
    "        wa_output_drop_rate=0.0,\n",
    "        ff_drop_rate=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [SwinBlock(\n",
    "                        dim=in_channels, \n",
    "                        heads=num_heads, \n",
    "                        head_dim=head_dim, \n",
    "                        mlp_dim=in_channels * 4,\n",
    "                        shifted=False, \n",
    "                        window_size=window_size, \n",
    "                        relative_pos_embedding=relative_pos_embedding,\n",
    "                        attention_drop_rate=attention_drop_rate, \n",
    "                        wa_output_drop_rate=wa_output_drop_rate, \n",
    "                        ff_drop_rate=ff_drop_rate\n",
    "                    ), \n",
    "                     SwinBlock(\n",
    "                         dim=in_channels, \n",
    "                         heads=num_heads, \n",
    "                         head_dim=head_dim, \n",
    "                         mlp_dim=in_channels * 4, \n",
    "                         shifted=True, \n",
    "                         window_size=window_size, \n",
    "                         relative_pos_embedding=relative_pos_embedding,\n",
    "                         attention_drop_rate=attention_drop_rate, \n",
    "                         wa_output_drop_rate=wa_output_drop_rate, \n",
    "                         ff_drop_rate=ff_drop_rate\n",
    "                     )]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.patch_partition = PatchMerging(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=hidden_dimension, \n",
    "            downscaling_factor=downscaling_factor\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x = regular_block(x)\n",
    "            x = shifted_block(x)\n",
    "        \n",
    "        x = x.permute(0, 4, 1, 2, 3) # (4, 56, 56, 56, 96) -> (4, 96, 56, 56, 56)\n",
    "        return self.patch_partition(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a14fb9-e54a-4578-9897-93702a31b73a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def double_conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "        nn.InstanceNorm3d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv3d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "        nn.InstanceNorm3d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class SwinTransformerUNet3D(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_dim=48, \n",
    "        heads=(3,3,3,3), \n",
    "        channels=1, \n",
    "        num_classes=3, \n",
    "        head_dim=32, \n",
    "        window_size=4, \n",
    "        layers=(2, 2, 2, 2), \n",
    "        downscaling_factors=(2, 2, 2, 2, 2), \n",
    "        relative_pos_embedding=True,\n",
    "        up_drop_rate=0.1,\n",
    "        attention_drop_rate=0.1,\n",
    "        wa_output_drop_rate=0.1,\n",
    "        ff_drop_rate=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_partition = PatchMerging(\n",
    "            in_channels=channels, \n",
    "            out_channels=hidden_dim, \n",
    "            downscaling_factor=downscaling_factors[0]\n",
    "        )\n",
    "        self.stage1 = StageModule(\n",
    "            in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[0], \n",
    "            downscaling_factor=downscaling_factors[1], num_heads=heads[0], head_dim=head_dim, \n",
    "            window_size=window_size, relative_pos_embedding=relative_pos_embedding,\n",
    "            attention_drop_rate=attention_drop_rate, wa_output_drop_rate=wa_output_drop_rate, ff_drop_rate=ff_drop_rate\n",
    "        )\n",
    "        # input shape\n",
    "        self.stage2 = StageModule(\n",
    "            in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[1],\n",
    "            downscaling_factor=downscaling_factors[2], num_heads=heads[1], head_dim=head_dim,\n",
    "            window_size=window_size, relative_pos_embedding=relative_pos_embedding,\n",
    "            attention_drop_rate=attention_drop_rate, wa_output_drop_rate=wa_output_drop_rate, ff_drop_rate=ff_drop_rate\n",
    "        )\n",
    "\n",
    "        self.stage3 = StageModule(\n",
    "            in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[2],\n",
    "            downscaling_factor=downscaling_factors[3], num_heads=heads[2], head_dim=head_dim,\n",
    "            window_size=window_size, relative_pos_embedding=relative_pos_embedding,\n",
    "            attention_drop_rate=attention_drop_rate, wa_output_drop_rate=wa_output_drop_rate, ff_drop_rate=ff_drop_rate\n",
    "        )\n",
    "\n",
    "        self.stage4 = StageModule(\n",
    "            in_channels=hidden_dim * 8, hidden_dimension=hidden_dim * 16, layers=layers[3],\n",
    "            downscaling_factor=downscaling_factors[4], num_heads=heads[3], head_dim=head_dim,\n",
    "            window_size=window_size, relative_pos_embedding=relative_pos_embedding,\n",
    "            attention_drop_rate=attention_drop_rate, wa_output_drop_rate=wa_output_drop_rate, ff_drop_rate=ff_drop_rate\n",
    "        )\n",
    "        \n",
    "        self.stage4_up = nn.ModuleList(\n",
    "            [double_conv_block(hidden_dim * 16, hidden_dim * 16), \n",
    "             nn.ConvTranspose3d(hidden_dim * 16, hidden_dim * 8, kernel_size=2, stride=2)]\n",
    "        )\n",
    "        \n",
    "        self.stage3_up = nn.ModuleList(\n",
    "            [double_conv_block(hidden_dim * 8, hidden_dim * 8), \n",
    "             nn.Dropout(up_drop_rate),\n",
    "             double_conv_block(hidden_dim * 16, hidden_dim * 8), \n",
    "             nn.ConvTranspose3d(hidden_dim * 8, hidden_dim * 4, kernel_size=2, stride=2)]\n",
    "        )\n",
    "        \n",
    "        self.stage2_up = nn.ModuleList(\n",
    "            [double_conv_block(hidden_dim * 4, hidden_dim * 4), \n",
    "             nn.Dropout(up_drop_rate),\n",
    "             double_conv_block(hidden_dim * 8, hidden_dim * 4), \n",
    "             nn.ConvTranspose3d(hidden_dim * 4, hidden_dim * 2, kernel_size=2, stride=2)]\n",
    "        )\n",
    "        \n",
    "        self.stage1_up = nn.ModuleList(\n",
    "            [double_conv_block(hidden_dim * 2, hidden_dim * 2), \n",
    "             nn.Dropout(up_drop_rate),\n",
    "             double_conv_block(hidden_dim * 4, hidden_dim * 2), \n",
    "             nn.ConvTranspose3d(hidden_dim * 2, hidden_dim, kernel_size=2, stride=2)]\n",
    "        )\n",
    "        self.final_up = nn.ModuleList(\n",
    "            [double_conv_block(hidden_dim, hidden_dim), \n",
    "             nn.Dropout(up_drop_rate),\n",
    "             double_conv_block(hidden_dim * 2, hidden_dim), \n",
    "             nn.ConvTranspose3d(hidden_dim, hidden_dim, kernel_size=2, stride=2)]\n",
    "        )\n",
    "        \n",
    "        self.image_size_blocks = nn.ModuleList(\n",
    "            [nn.Linear(channels, hidden_dim), double_conv_block(hidden_dim * 2, hidden_dim)]\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Sequential(nn.Conv3d(hidden_dim, num_classes, kernel_size=1),  nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        # width, height, and depth, each should be divisible by product of downscaling factors and window size \n",
    "        # (width, height, or deapth) % (np.prod(downscaling factors) * window_size) == 0\n",
    "        \n",
    "        # image shape(b, 3, 128, 128, 128) \n",
    "        x0 = self.patch_partition(img)  # (b, 64, 64, 64, 48)\n",
    "        x1 = self.stage1(x0)  # (b, 32, 32, 32, 96)\n",
    "        x2 = self.stage2(x1)  # (b, 16, 16, 16, 192)\n",
    "        x3 = self.stage3(x2)  # (b, 8, 8, 8, 384)\n",
    "        x4 = self.stage4(x3)  # (b, 4, 4, 4, 768)\n",
    "        \n",
    "        x = self.stage4_up[0](x4.permute(0, 4, 1, 2, 3))  # double conv block (b, 768, 4, 4, 4)\n",
    "        x = self.stage4_up[1](x)   # transposed convolution to increase spatial size (b, 384, 8, 8, 8)\n",
    "        \n",
    "        x = self.stage3_up[0](x)  # double conv block (b, 384, 8, 8, 8)\n",
    "        x = torch.cat((x, x3.permute(0, 4, 1, 2, 3)), dim=1)   # (b, 768, 8, 8, 8)\n",
    "        x = self.stage3_up[1](x)  # dropout\n",
    "        x = self.stage3_up[2](x)  # double conv block with skip connection (b, 384, 8, 8, 8)\n",
    "        x = self.stage3_up[3](x)  # transposed convolution to increase spatial size (b, 192, 16, 16, 16)\n",
    "        \n",
    "        x = self.stage2_up[0](x)  # double conv block (b, 192, 16, 16, 16)\n",
    "        x = torch.cat((x, x2.permute(0, 4, 1, 2, 3)), dim=1)   # (b, 384, 16, 16, 16)\n",
    "        x = self.stage2_up[1](x)  # dropout\n",
    "        x = self.stage2_up[2](x)  # double conv block with skip connection (b, 192, 16, 16, 16)\n",
    "        x = self.stage2_up[3](x)  # transposed convolution to increase spatial size (b, 96, 32, 32, 32)\n",
    "        \n",
    "        x = self.stage1_up[0](x)  # double conv block (b, 96, 32, 32, 32)\n",
    "        x = torch.cat((x, x1.permute(0, 4, 1, 2, 3)), dim=1)  # (b, 192, 32, 32, 32)\n",
    "        x = self.stage1_up[1](x)  # dropout\n",
    "        x = self.stage1_up[2](x)  # double conv block with skip connection (b, 96, 32, 32, 32)\n",
    "        x = self.stage1_up[3](x)  # transposed convolution to increase spatial size (b, 48, 64, 64, 64)\n",
    "        \n",
    "        x = self.final_up[0](x)    # double conv block (b, 48, 64, 64, 64)\n",
    "        x = torch.cat((x, x0.permute(0, 4, 1, 2, 3)), dim=1)   # (b, 96, 64, 64, 64)\n",
    "        x = self.final_up[1](x)  # dropout\n",
    "        x = self.final_up[2](x)  # transposed convolution to increase spatial size (b, 48, 64, 64, 64)\n",
    "        x = self.final_up[3](x)  # transposed convolution to increase spatial size (b, 48, 128, 128, 128)\n",
    "        \n",
    "        image_embedded = self.image_size_blocks[0](img.permute(0, 2, 3, 4, 1)).permute(0, 4, 1, 2 ,3)   # (b, 48, 128, 128, 128)\n",
    "        x = torch.cat((x, image_embedded), dim=1)   # (b, 96, 128, 128, 128)\n",
    "        x = self.image_size_blocks[1](x)   # (b, 48, 128, 128, 128)\n",
    "        \n",
    "        return self.final_conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=3, features =[64, 128, 256, 512]):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        #down part of UNet\n",
    "        for feature in features:\n",
    "            self.downs.append(double_conv_block(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        #upsample part of UNet\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "              nn.ConvTranspose3d(feature*2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(double_conv_block(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = double_conv_block(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Sequential(nn.Conv3d(features[0], out_channels, kernel_size=1),  nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            # if x.shape != skip_connection.shape:\n",
    "            #     x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b2d9e-8460-4fd9-a236-1a3e9ae3bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = SwinTransformerUNet3D()\n",
    "    model.to(CFG.device)\n",
    "    return model\n",
    "\n",
    "def load_model(path):\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294608a-abf9-4089-921c-34788129e070",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21337543-ce88-492a-86bd-50c1c9e38749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmtation_indices(mask):\n",
    "    \"\"\"\n",
    "    :mask: (C x W x H x D) Tensor of the probability map of the estimation.\n",
    "    :return: List of List of Tensors of the Ground Truth points.\n",
    "                   The outer most list must be of size B as in prob_map.\n",
    "                   The second outer most list must be of size C.\n",
    "                   Each element in the second outer most list must be a 2D Tensor,\n",
    "                   where each row is the (x, y, z), i.e, (col, row, depth) of a GT point.\n",
    "    \"\"\"\n",
    "    num_classes = mask.size()[0]\n",
    "    mask_indices = []\n",
    "    for i in range(num_classes):\n",
    "        mask_indices.append((mask[i] > 0).nonzero())\n",
    "    \n",
    "    return mask_indices\n",
    "\n",
    "\n",
    "def _assert_no_grad(variables_list):\n",
    "    for variables in variables_list:\n",
    "        for var in variables:\n",
    "            assert not var.requires_grad, \\\n",
    "                \"nn criterions don't compute the gradient w.r.t. targets - please \" \\\n",
    "                \"mark these variables as volatile or not requiring gradients\"\n",
    "\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        super(DiceLoss, self).__init__()\n",
    "    def forward(self, pred, target):\n",
    "        batch_size = pred.shape[0]\n",
    "        smooth = 0.001\n",
    "        iflat = pred.contiguous().view(batch_size, -1)\n",
    "        tflat = target.contiguous().view(batch_size, -1)\n",
    "        intersection = (iflat * tflat).sum(-1)\n",
    "        A_sum = torch.sum(iflat * iflat, -1)\n",
    "        B_sum = torch.sum(tflat * tflat, -1)\n",
    "        return (1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )).mean()\n",
    "\n",
    "\n",
    "class WeightedHausdorffLoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes, resized_shape, alpha=-4.0, return_by_class=False, device=torch.device('cpu')):\n",
    "        self.alpha = alpha\n",
    "        self.return_by_class = return_by_class\n",
    "        self.width, self.height, self.depth = resized_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.resized_size = torch.tensor(\n",
    "            resized_shape,\n",
    "            dtype=torch.get_default_dtype(), \n",
    "            device=device\n",
    "        )\n",
    "        self.n_pixels = np.prod(resized_shape)\n",
    "        \n",
    "        self.all_pixel_locations = torch.from_numpy(\n",
    "            np.indices(resized_shape, dtype=np.int32).reshape(3,-1).transpose()\n",
    "        ).to(device=device, dtype=torch.get_default_dtype())\n",
    "        self.const_one = torch.tensor(1.0).to(device=device, dtype=torch.get_default_dtype())\n",
    "        super(WeightedHausdorffLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, prob_map, gt, orig_sizes):\n",
    "        \"\"\"\n",
    "        Compute the Weighted Hausdorff Distance function\n",
    "        between the estimated probability map and ground truth points.\n",
    "        The output is the WHD averaged through all the batch.\n",
    "        :param prob_map: (B x C x W x H x D) Tensor of the probability map of the estimation.\n",
    "                         B is batch size, C is number of classes, H is height, W is width and D is depth.\n",
    "                         Values must be between 0 and 1.\n",
    "        :param gt: List of List of Tensors of the Ground Truth points.\n",
    "                   The outer most list must be of size B as in prob_map.\n",
    "                   The second outer most list must be of size C.\n",
    "                   Each element in the second outer most list must be a 2D Tensor,\n",
    "                   where each row is the (x, y, z), i.e, (col, row, depth) of a GT point.\n",
    "        :param orig_sizes: Bx3 Tensor containing the size\n",
    "                           of the original images.\n",
    "                           B is batch size.\n",
    "                           The size must be in (width, height, depth) format.\n",
    "        :return: Single-scalar Tensor with the Weighted Hausdorff Distance.\n",
    "                 If self.return_2_terms=True, then return a tuple containing\n",
    "                 the two terms of the Weighted Hausdorff Distance.\n",
    "        \"\"\"\n",
    "\n",
    "        _assert_no_grad(gt)\n",
    "\n",
    "        assert prob_map.dim() == 5, 'The probability map shape must be (B x C x W x H x D)'\n",
    "        assert prob_map.size()[1:5] == (self.num_classes, self.width, self.height, self.depth), \\\n",
    "        f'prob_map size is {prob_map.size()[1:5]}, it must have size {(self.num_classes, self.width, self.height, self.depth)}'\n",
    "\n",
    "        batch_size = prob_map.shape[0]\n",
    "        assert batch_size == len(gt)\n",
    "\n",
    "        distances_by_class = [[] for _ in range(self.num_classes)]\n",
    "    \n",
    "        for batch_index in range(batch_size):\n",
    "            orig_size_b = orig_sizes[batch_index, :]\n",
    "            norm_factor = (orig_size_b/self.resized_size)\n",
    "            max_dist = (orig_size_b ** 2).sum().sqrt()\n",
    "            for class_index in range(self.num_classes):\n",
    "                # One by one\n",
    "                prob_map_b = prob_map[batch_index, class_index]\n",
    "                gt_b = gt[batch_index][class_index]\n",
    "\n",
    "                # Corner case: no GT points\n",
    "                if gt_b.size()[0] == 0:\n",
    "                    # term_1 = max_dist\n",
    "                    # term_2 = 0\n",
    "                    distances_by_class[class_index].append(self.const_one)\n",
    "                    continue\n",
    "\n",
    "                # Pairwise distances between all possible locations and the GTed locations\n",
    "                n_gt_pts = gt_b.size()[0]\n",
    "                \n",
    "                # normalized_x has shape (width * height * depth, 3)\n",
    "                # normalized_y has shape (number of masked points, 3)\n",
    "                normalized_x = norm_factor * self.all_pixel_locations\n",
    "                normalized_y = norm_factor * gt_b\n",
    "\n",
    "                # normalized_x.unsqueeze(1) has shape (width * height * depth, 1, 3)\n",
    "                # normalized_y.unsqueeze(0) has shape (1, number of true segmented points, 3)\n",
    "                # diff has shape (width * height * depth, number of true segmented points, 3)\n",
    "                diffs = normalized_x.unsqueeze(1) - normalized_y.unsqueeze(0)\n",
    "                \n",
    "                # distances has shape (width * height * depth, number of true segmented points)\n",
    "                # normalize the distance with max distance \n",
    "                distance_matrix = torch.sum(diffs ** 2, -1).sqrt() / max_dist\n",
    "\n",
    "                # Reshape probability map as a long column vector,\n",
    "                # and prepare it for multiplication\n",
    "                p = prob_map_b.view(prob_map_b.nelement())\n",
    "                total_x_weight = p.sum()\n",
    "\n",
    "                # Weighted Hausdorff Distance\n",
    "                term_1 = (1 / (total_x_weight + 1e-6)) * torch.sum(p * torch.min(distance_matrix, 1)[0])\n",
    "                \n",
    "                p = p.view(-1, 1)\n",
    "                weighted_distance_matrix = (1 - p) + p*distance_matrix\n",
    "                \n",
    "                # get generalized mean\n",
    "                generalized_mean = torch.mean((weighted_distance_matrix + 1e-6)**self.alpha, 0)**(1./self.alpha)\n",
    "                term_2 = torch.mean(generalized_mean)\n",
    "                \n",
    "                result = term_1 + term_2\n",
    "                distances_by_class[class_index].append(result)\n",
    "\n",
    "        result = torch.stack(\n",
    "            [distances_by_class[class_index][batch_index] for batch_index in range(batch_size) for class_index in range(self.num_classes)]\n",
    "        ).view(batch_size, self.num_classes)\n",
    "        \n",
    "        if self.return_by_class:\n",
    "            return result.mean(dim=0) \n",
    "\n",
    "        return result.mean(dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efdf41-dfdb-4952-95b5-a7adf7b9daaa",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b6c04b0-7fad-4c70-a769-65998fae275c",
   "metadata": {},
   "source": [
    "train_loader, valid_loader = prepare_loaders(df_train, fold=0, debug=True)\n",
    "inputs = None\n",
    "targets = None\n",
    "case_days = None\n",
    "original_sizes = None\n",
    "subjects = None\n",
    "for subjects_batch in train_loader:\n",
    "    subjects = subjects_batch\n",
    "    inputs = subjects_batch['scan'][tio.DATA]\n",
    "    targets = subjects_batch['label'][tio.DATA]\n",
    "    case_days = subjects_batch['case_day']\n",
    "    original_sizes = subjects_batch['original_size']\n",
    "\n",
    "    break\n",
    "\n",
    "x = subjects_batch['scan'][tio.DATA]\n",
    "y = subjects_batch['label'][tio.DATA]\n",
    "case_days = subjects_batch['case_day']\n",
    "original_sizes = torch.stack(subjects_batch['original_size']).transpose(0,1).to(CFG.device)\n",
    "    \n",
    "model = SwinTransformerUNet3D()\n",
    "model.to(CFG.device)\n",
    "x = x.to(CFG.device)\n",
    "y = y.to(CFG.device)\n",
    "preds = model(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "441ad104-a3fe-411f-a44c-d30f48d967ef",
   "metadata": {},
   "source": [
    "mask_max_pool = nn.functional.max_pool3d(y, CFG.whf_pool_size)\n",
    "segmentation_indices = []\n",
    "for i in range(CFG.train_bs):\n",
    "    class_segmentation_indices = [item.to(CFG.device) for item in get_segmtation_indices(mask_max_pool[i])]\n",
    "    segmentation_indices.append(class_segmentation_indices)\n",
    "\n",
    "whf_loss = WeightedHausdorffLoss(3, [val // CFG.whf_pool_size for val in CFG.image_size], alpha=-1.0, device=CFG.device)\n",
    "dice_loss = DiceLoss(3)\n",
    "loss = 0.4 * dice_loss(preds, y)  + 0.6 * whf_loss(nn.functional.avg_pool3d(preds, CFG.whf_pool_size), segmentation_indices, original_sizes)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8e4bb-1556-4ab1-ac68-8f9d40088d98",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1341151-9a7f-4739-a401-01b454e30fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch, whf):\n",
    "    model.train()\n",
    "    dice = DiceLoss(3)\n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    running_dice_loss = 0.0\n",
    "    running_whf_loss = 0.0\n",
    "    total_size = len(dataloader)\n",
    "    pbar = tqdm(enumerate(dataloader), total=total_size, desc='Train ', ncols=150)\n",
    "    \n",
    "    last_n_accumulate = total_size % CFG.n_accumulate\n",
    "    last_n_accumulate_begin = total_size - last_n_accumulate\n",
    "    \n",
    "    for step, subjects_batch in pbar:\n",
    "        images = subjects_batch['scan'][tio.DATA]\n",
    "        masks = subjects_batch['label'][tio.DATA]\n",
    "        original_sizes = subjects_batch['original_size']\n",
    "        original_sizes = torch.stack(subjects_batch['original_size']).transpose(0,1)\n",
    "        case_days = subjects_batch['case_day']\n",
    "        \n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        masks  = masks.to(device, dtype=torch.float)\n",
    "        original_sizes = original_sizes.to(device, dtype=torch.float)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        mask_max_pool = nn.functional.max_pool3d(masks,CFG.whf_pool_size)\n",
    "        segmentation_indices = []\n",
    "        for i in range(batch_size):\n",
    "            class_segmentation_indices = [item.to(CFG.device) for item in get_segmtation_indices(mask_max_pool[i])]\n",
    "            segmentation_indices.append(class_segmentation_indices)\n",
    "        \n",
    "        with amp.autocast(enabled=True):\n",
    "            y_preds = model(images)\n",
    "            dice_loss = dice(y_preds, masks)\n",
    "            whf_loss = whf(nn.functional.avg_pool3d(y_preds, CFG.whf_pool_size), segmentation_indices, original_sizes)\n",
    "            loss = 0.4 * dice_loss + 0.6 * whf_loss\n",
    "        \n",
    "        if step >= last_n_accumulate_begin:\n",
    "            current_n_accumulate = last_n_accumulate\n",
    "            need_update = (step % CFG.n_accumulate) + 1 == last_n_accumulate\n",
    "        else:\n",
    "            current_n_accumulate = CFG.n_accumulate\n",
    "            need_update =  (step + 1) % CFG.n_accumulate == 0\n",
    "            \n",
    "        scaler.scale(loss / current_n_accumulate).backward()\n",
    "    \n",
    "        if need_update:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        running_dice_loss += (dice_loss.item() * batch_size)\n",
    "        running_whf_loss += (whf_loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_dice_loss = running_dice_loss / dataset_size\n",
    "        epoch_whf_loss = running_whf_loss / dataset_size\n",
    "        \n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}', dice_l=f'{epoch_dice_loss:0.4f}', whf_l=f'{epoch_whf_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}',\n",
    "                        gpu_mem=f'{mem:0.2f} GB')\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08882f-1c41-4c9b-8c83-511dd8ccdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, thr=0.5, dim=(2,3,4), epsilon=0.001):\n",
    "    y_true = y_true.to(torch.float32)\n",
    "    y_pred = (y_pred>thr).to(torch.float32)\n",
    "    inter = (y_true*y_pred).sum(dim=dim)\n",
    "    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n",
    "    dice = ((2*inter+epsilon)/(den+epsilon)).mean(dim=(1,0))\n",
    "    return dice\n",
    "\n",
    "def iou_coef(y_true, y_pred, thr=0.5, dim=(2,3, 4), epsilon=0.001):\n",
    "    y_true = y_true.to(torch.float32)\n",
    "    y_pred = (y_pred>thr).to(torch.float32)\n",
    "    inter = (y_true*y_pred).sum(dim=dim)\n",
    "    union = (y_true + y_pred - y_true*y_pred).sum(dim=dim)\n",
    "    iou = ((inter+epsilon)/(union+epsilon)).mean(dim=(1,0))\n",
    "    return iou\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch, whf):\n",
    "    model.eval()\n",
    "    dice = DiceLoss(3)\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    running_dice_loss = 0.0\n",
    "    running_whf_loss = 0.0\n",
    "    \n",
    "    val_scores = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ', ncols=150)\n",
    "   \n",
    "    for step, subjects_batch in pbar:\n",
    "        images = subjects_batch['scan'][tio.DATA]\n",
    "        masks = subjects_batch['label'][tio.DATA]\n",
    "        original_sizes = subjects_batch['original_size']\n",
    "        original_sizes = torch.stack(subjects_batch['original_size']).transpose(0,1)\n",
    "        case_days = subjects_batch['case_day']\n",
    "\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        masks  = masks.to(device, dtype=torch.float)\n",
    "        original_sizes = original_sizes.to(device, dtype=torch.float)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        mask_max_pool = nn.functional.max_pool3d(masks,CFG.whf_pool_size)\n",
    "        segmentation_indices = []\n",
    "        for i in range(batch_size):\n",
    "            class_segmentation_indices = [item.to(CFG.device) for item in get_segmtation_indices(mask_max_pool[i])]\n",
    "            segmentation_indices.append(class_segmentation_indices)\n",
    "        \n",
    "        y_preds  = model(images)\n",
    "        dice_loss = dice(y_preds, masks)\n",
    "        whf_loss = whf(nn.functional.avg_pool3d(y_preds, CFG.whf_pool_size), segmentation_indices, original_sizes)\n",
    "        loss = 0.4 * dice_loss + 0.6 * whf_loss\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        running_dice_loss += (dice_loss.item() * batch_size)\n",
    "        running_whf_loss += (whf_loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_dice_loss = running_dice_loss / dataset_size\n",
    "        epoch_whf_loss = running_whf_loss / dataset_size\n",
    "        \n",
    "        # y_pred = nn.Sigmoid()(y_pred)\n",
    "        val_dice = dice_coef(masks, y_preds).cpu().detach().numpy()\n",
    "        val_jaccard = iou_coef(masks, y_preds).cpu().detach().numpy()\n",
    "        val_scores.append([val_dice, val_jaccard])\n",
    "        \n",
    "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
    "        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}', dice_l=f'{epoch_dice_loss:0.4f}', whf_l=f'{epoch_whf_loss:0.4f}',\n",
    "                        gpu_memory=f'{mem:0.2f} GB')\n",
    "    val_scores = np.mean(val_scores, axis=0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fc259-35ee-4c51-a115-84969917e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, model, whf_loss_func, optimizer, scheduler, device, num_epochs):\n",
    "    # To automatically log gradients\n",
    "    # wandb.watch(model, log_freq=100)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss      = np.inf\n",
    "    best_epoch     = -1\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
    "        train_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CFG.device, epoch=epoch, whf=whf_loss_func)\n",
    "        \n",
    "        val_loss, val_scores = valid_one_epoch(model, valid_loader, \n",
    "                                                 device=CFG.device, \n",
    "                                                 epoch=epoch, whf=whf_loss)\n",
    "        val_dice, val_jaccard = val_scores\n",
    "    \n",
    "        history['Train Loss'].append(train_loss)\n",
    "        history['Valid Loss'].append(val_loss)\n",
    "        history['Valid Dice'].append(val_dice)\n",
    "        history['Valid Jaccard'].append(val_jaccard)\n",
    "        \n",
    "        # Log the metrics\n",
    "        \"\"\"\n",
    "        wandb.log({\"Train Loss\": train_loss, \n",
    "                   \"Valid Loss\": val_loss,\n",
    "                   \"Valid Dice\": val_dice,\n",
    "                   \"Valid Jaccard\": val_jaccard,\n",
    "                   \"LR\":scheduler.get_last_lr()[0]})\n",
    "        \"\"\"\n",
    "        print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f} | Valid Loss: {val_loss:0.8f}')\n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_loss <= best_loss:\n",
    "            print(f\"Valid loss Improved ({best_loss:0.4f} ---> {val_loss:0.4f})\")\n",
    "            best_loss    = val_loss\n",
    "            best_jaccard = val_jaccard\n",
    "            best_dice = val_dice\n",
    "            best_epoch   = epoch\n",
    "            \"\"\"\n",
    "            run.summary[\"Best Dice\"]    = best_dice\n",
    "            run.summary[\"Best Jaccard\"] = best_jaccard\n",
    "            run.summary[\"Best Epoch\"]   = best_epoch\n",
    "            \"\"\"\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_path = f\"best_epoch-{fold:02d}.bin\"\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            # Save a model file from the current directory\n",
    "            # wandb.save(PATH)\n",
    "            print(f\"Model Saved\")\n",
    "            \n",
    "        last_model_wts = copy.deepcopy(model.state_dict())\n",
    "        model_path = f\"last_epoch-{fold:02d}.bin\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "        print(); print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe660c4-7b0a-4994-bfa9-71e80756fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_scheduler(optimizer):\n",
    "    if CFG.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max, \n",
    "                                                   eta_min=CFG.min_lr)\n",
    "    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CFG.T_0, \n",
    "                                                             eta_min=CFG.min_lr)\n",
    "    elif CFG.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                   mode='min',\n",
    "                                                   factor=0.1,\n",
    "                                                   patience=7,\n",
    "                                                   threshold=0.0001,\n",
    "                                                   min_lr=CFG.min_lr,)\n",
    "    elif CFG.scheduer == 'ExponentialLR':\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n",
    "    elif CFG.scheduler == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c649ee-6045-4480-8f48-09f66093ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "whf_loss = WeightedHausdorffLoss(3, [val // CFG.whf_pool_size for val in CFG.image_size], alpha=-5.0, device=CFG.device)\n",
    "for fold in range(1):\n",
    "    print(f'#'*15)\n",
    "    print(f'### Fold: {fold}')\n",
    "    print(f'#'*15)\n",
    "    \"\"\"\n",
    "    run = wandb.init(project='uw-maddison-gi-tract', \n",
    "                     config={k:v for k, v in dict(vars(CFG)).items() if '__' not in k},\n",
    "                     anonymous=anonymous,\n",
    "                     name=f\"fold-{fold}|dim-{CFG.img_size[0]}x{CFG.img_size[1]}|model-{CFG.model_name}\",\n",
    "                     group=CFG.comment,\n",
    "                    )\n",
    "    \"\"\"\n",
    "    train_loader, valid_loader = prepare_loaders(df_train, fold=fold, debug=CFG.debug)\n",
    "    # model = build_model()\n",
    "    model = load_model(f\"best_epoch-{fold:02d}.bin\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n",
    "    scheduler = fetch_scheduler(optimizer)\n",
    "    model, history = run_training(fold, model, whf_loss, optimizer, scheduler,\n",
    "                                  device=CFG.device,\n",
    "                                  num_epochs=CFG.epochs)\n",
    "    \"\"\"\n",
    "    run.finish()\n",
    "    display(ipd.IFrame(run.url, width=1000, height=720))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4e5d6-4204-43ac-8f97-c5da1b33ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img, mask=None):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "#     img = clahe.apply(img)\n",
    "#     plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img, cmap='bone')\n",
    "    \n",
    "    if mask is not None:\n",
    "        # plt.imshow(np.ma.masked_where(mask!=1, mask), alpha=0.5, cmap='autumn')\n",
    "        plt.imshow(mask, alpha=0.5)\n",
    "        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n",
    "        labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n",
    "        plt.legend(handles,labels)\n",
    "    plt.axis('off')\n",
    "\n",
    "def plot_batch(imgs, msks, depth, size=3):\n",
    "    plt.figure(figsize=(5*5, 5))\n",
    "    for idx in range(size):\n",
    "        plt.subplot(1, 5, idx+1)\n",
    "        img = imgs[idx, :, :, :, depth].permute((1, 2, 0)).numpy()*255.0\n",
    "        img = img.astype('uint8')\n",
    "        msk = msks[idx, :, :, :, depth].permute((1, 2, 0)).numpy()*255.0\n",
    "        print(msk.sum())\n",
    "        show_img(img, msk)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8edfe-626a-45e7-9965-55f21fafc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "test_dataset = tio.SubjectsDataset(\n",
    "        [get_subject(row['case_day'], row['image_path'], row['mask_path']) for _, row in df_train.query(\"fold==0\").sample(frac=1.0).iterrows()], \n",
    "        transform=data_transforms['valid']\n",
    "    )\n",
    "\n",
    "#test_loader  = DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a2aac-1a30-46f4-841a-b762ae36b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = test_dataset[11]\n",
    "image = subject['scan'][tio.DATA]\n",
    "image = image.unsqueeze(0).to(CFG.device, dtype=torch.float)\n",
    "mask = subject['label'][tio.DATA]\n",
    "mask = mask.unsqueeze(0).to(CFG.device, dtype=torch.float)\n",
    "\n",
    "pred = None\n",
    "fold=0\n",
    "model = load_model(f\"best_epoch-{fold:02d}.bin\")\n",
    "with torch.no_grad():\n",
    "    pred = model(image)\n",
    "    pred = (pred>0.2).double()\n",
    "    \n",
    "image  = image.cpu().detach()\n",
    "mask = mask.cpu().detach()\n",
    "pred = pred.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f33388-8f26-461e-a6f2-9f7086d4f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aac27a-85ef-4d00-a756-1ea954e8ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth=66\n",
    "plot_batch(image, pred, depth=depth, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c3f31-fada-400d-a7e1-f2c20ae03a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(image, mask, depth=depth, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae133c-c1b3-4e50-ac44-01b152e32426",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(2,1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n",
    "scheduler = fetch_scheduler(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f675b1f-308a-4d82-83e0-e1989f088d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = []\n",
    "steps = []\n",
    "for i in range(16 * 300):\n",
    "    optimizer.step()\n",
    "    if i % 300 == 0:\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        steps.append(i)\n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "plt.plot(steps, lrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
